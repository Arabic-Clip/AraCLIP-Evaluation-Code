{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5JU97PayTvv"
      },
      "outputs": [],
      "source": [
        "# !conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=10.2 -c pytorch\n",
        "# !pip install ftfy==5.8\n",
        "# !conda install transformers\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, random\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import clip\n",
        "# !pip install ipywidgets\n",
        "# !git clone https://github.com/FreddeFrallan/Multilingual-CLIP\n",
        "# !cd Multilingual-CLIP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validation pipeline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch \n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from rich import print "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the images from of the dataset \n",
        "import os \n",
        "\n",
        "img_folder = 'photos/'\n",
        "\n",
        "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
        "    os.makedirs(img_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json \n",
        "\n",
        "data = [] \n",
        "\n",
        "with open(\"en_ar_XTD10_edited_v2.jsonl\") as filino:\n",
        "\n",
        "    for file_i in filino:\n",
        "\n",
        "        dic_obj = json.loads(file_i)\n",
        "        data.append(dic_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dataset size is: \", len(data) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Check_id_duplication = [] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, data_obj in enumerate(data):\n",
        "\n",
        "    Check_id_duplication.append(data_obj[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If the len is 1000, there is no duplicates\n",
        "\n",
        "len(set(Check_id_duplication)) == 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = [\n",
        "#     {'image_id': 0, 'id': 391895, 'caption': 'رجل يرتدي خوذة حمراء على دراجة بخارية صغيرة على طريق ترابي'},\n",
        "#     {'image_id': 1, 'id': 522418, 'caption': 'امرأة ترتدي شبكة على رأسها تقطع كعكة'},\n",
        "#     {'image_id': 2, 'id': 184613, 'caption': 'طفل يحمل مظلة مزهرة ويأكل ثورًا'},\n",
        "# ]\n",
        "\n",
        "# Sort the list of dictionaries based on the 'id' key\n",
        "sorted_data = sorted(data, key=lambda x: x['id'])\n",
        "\n",
        "print(sorted_data[:20])\n",
        "# # Print the sorted list\n",
        "# for item in sorted_data:\n",
        "#     print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get only 10 examples\n",
        "# sorted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(sorted_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sorted_data[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_name_list = []\n",
        "\n",
        "for lin in sorted_data:\n",
        "    # print(lin[\"image_name\"])\n",
        "    image_name_list.append(lin[\"image_name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(image_name_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a mapping dictionary between the ids and paths\n",
        "\n",
        "id2path = {}\n",
        "\n",
        "\n",
        "for im_path, sort_sample in zip(image_name_list, sorted_data):\n",
        "\n",
        "\n",
        "    # print(json.loads(lin)[\"text\"])\n",
        "    # print(im_path.split(\"_\")[-1].split(\".\")[0])\n",
        "\n",
        "    input_str = im_path.split(\"_\")[-1].split(\".\")[0]\n",
        "    # print(input_str)\n",
        "    result = int(input_str.lstrip('0'))\n",
        "    # Check the ids\n",
        "    if sort_sample['id'] != result:\n",
        "        print(\"stop ........................................................\")\n",
        "    id2path[result] = im_path\n",
        "\n",
        "    # print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if each image file exists in the folder\n",
        "\n",
        "folder_path = \"photos/XTD10_dataset\"\n",
        "\n",
        "missing_images = []\n",
        "\n",
        "for image_path in image_name_list:\n",
        "    full_image_path = os.path.join(folder_path, image_path)\n",
        "    if not os.path.exists(full_image_path):\n",
        "        missing_images.append(image_path)\n",
        "\n",
        "if missing_images:\n",
        "    print(\"The following images are missing:\")\n",
        "    for image_path in missing_images:\n",
        "        print(image_path)\n",
        "else:\n",
        "    print(\"All images are present in the folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the images that are not included on the testing dataset \n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "not_exist_paths = []\n",
        "exist_paths = [] \n",
        "\n",
        "# Get a list of all files in the folder\n",
        "all_files = os.listdir(folder_path)\n",
        "\n",
        "# Remove any files in the folder that are not in the list of image paths\n",
        "for file_name in all_files:\n",
        "    if file_name not in image_name_list:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        os.remove(file_path)\n",
        "        # print(f\"Removed: {file_path}\")\n",
        "        not_exist_paths.append(file_path)\n",
        "\n",
        "    elif file_name in image_name_list:\n",
        "\n",
        "        exist_paths.append(file_name)\n",
        "\n",
        "\n",
        "destroy_images = set(not_exist_paths).difference(set(exist_paths))\n",
        "\n",
        "\n",
        "print(\"img_names\", len(all_files))\n",
        "print(\"destroy_images\", len(destroy_images))\n",
        "print(\"not_exist_paths\", len(not_exist_paths))\n",
        "print(\"remaining images\", len(all_files)- len(destroy_images))\n",
        "\n",
        "# print(\"Finished removing unwanted images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the the text model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "class MultilingualClipEdited(torch.nn.Module):\n",
        "    def __init__(self, model_name, tokenizer_name, head_name, weights_dir='data/weights/', cache_dir=None,in_features=None,out_features=None):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.head_path = weights_dir + head_name\n",
        "\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=cache_dir)\n",
        "        # print(self.tokenizer )\n",
        "        self.transformer = transformers.AutoModel.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "        self.clip_head = torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        self._load_head()\n",
        "\n",
        "    def forward(self, txt):\n",
        "        txt_tok = self.tokenizer(txt, padding=True, return_tensors='pt')\n",
        "        embs = self.transformer(**txt_tok)[0]\n",
        "        \n",
        "        # print(\"embs shape: \", embs.shape)\n",
        "\n",
        "        att = txt_tok['attention_mask']\n",
        "\n",
        "        # print(\"att shape: \", att.shape)\n",
        "    \n",
        "        embs = (embs * att.unsqueeze(2)).sum(dim=1) / att.sum(dim=1)[:, None]\n",
        "\n",
        "        # print(\"embs after att shape: \", embs.shape)\n",
        "\n",
        "        return self.clip_head(embs)\n",
        "\n",
        "    def _load_head(self):\n",
        "        with open(self.head_path, 'rb') as f:\n",
        "            lin_weights = pickle.loads(f.read())\n",
        "        self.clip_head.weight = torch.nn.Parameter(torch.tensor(lin_weights[0]).float().t())\n",
        "        self.clip_head.bias = torch.nn.Parameter(torch.tensor(lin_weights[1]).float())\n",
        "\n",
        "AVAILABLE_MODELS = {\n",
        "\n",
        "\n",
        "    'arabert-large-vit-B-16-plus-mscoc-60': {\n",
        "    'model_name': 'Arabic-Clip/arabertv2-Vit-B-16-plus-epoch-60-trained-mscoco-training',\n",
        "    'tokenizer_name': 'Arabic-Clip/arabertv2-Vit-B-16-plus-epoch-60-trained-mscoco-training',\n",
        "    'head_name': 'heads_of_the_model_bert-large-arabertv2-Vit-B-16-plus-240-60_.pickle'\n",
        "    },\n",
        "    'arabert-large-vit-B-16-plus-mscoc-60-32': {\n",
        "    'model_name': 'Arabic-Clip/arabertv2-Vit-B-16-plus-epoch-60-trained-mscoco-training-fp32',\n",
        "    'tokenizer_name': 'Arabic-Clip/arabertv2-Vit-B-16-plus-epoch-60-trained-mscoco-training-fp32',\n",
        "    'head_name': 'heads_of_the_model_bert-large-arabertv2-Vit-B-16-plus-240-60_32.pickle'\n",
        "    },\n",
        "\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(name, cache_dir=None,in_features=None,out_features=None):\n",
        "    config = AVAILABLE_MODELS[name]\n",
        "    print(config)\n",
        "    return MultilingualClipEdited(**config, cache_dir=cache_dir, in_features= in_features, out_features=out_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def download_file(url, folder_path, filename=None):\n",
        "    # If filename is not specified, use the last part of the URL as the filename\n",
        "    if filename is None:\n",
        "        filename = os.path.basename(url)\n",
        "    \n",
        "    # Full path where the file should be saved\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    \n",
        "    # Check if the file already exists\n",
        "    if not os.path.exists(file_path):\n",
        "        # Make sure the folder exists\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        \n",
        "        # Download the file\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check for HTTP request errors\n",
        "        \n",
        "        # Write the file to the specified path\n",
        "        with open(file_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        \n",
        "        print(f\"File downloaded and saved to {file_path}\")\n",
        "    else:\n",
        "        print(f\"File already exists at {file_path}\")\n",
        "\n",
        "# Example usage\n",
        "url = \"https://huggingface.co/Arabic-Clip-Archive/arabertv2-Vit-B-16-plus-epoch-60-trained-mscoco-training/resolve/main/heads_of_the_model_bert-large-arabertv2-Vit-B-16-plus-240-60_.pickle\"\n",
        "folder_path = \"data/weights/\"\n",
        "download_file(url, folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the pickle file in binary read mode\n",
        "\n",
        "pickle_file_path = 'data/weights/heads_of_the_model_bert-large-arabertv2-Vit-B-16-plus-240-60_.pickle'  # Replace with the actual path to your pickle file\n",
        "with open(pickle_file_path, 'rb') as file:\n",
        "    loaded_content = pickle.load(file)\n",
        "    print(len(loaded_content))\n",
        "    print(loaded_content[0].shape)\n",
        "    print(loaded_content[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Text model name \n",
        "# \n",
        "\n",
        "text_model = load_model('arabert-large-vit-B-16-plus-mscoc-60', in_features= 1024, out_features=640)\n",
        "\n",
        "\n",
        "# Define the language model with lambda \n",
        "\n",
        "language_model = lambda queries: np.asarray(text_model(queries).detach().to('cpu')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the image model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# clip_model, compose = clip.load('RN50x4')\n",
        "# import torch\n",
        "import open_clip\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "# from open_clip # import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Device: \", device)\n",
        "\n",
        "clip_model, _, compose = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
        "clip_model.to(device)\n",
        "\n",
        "\n",
        "# clip_model, compose = create_model_from_pretrained('hf-hub:timm/ViT-B-16-SigLIP-512')\n",
        "# tokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP-512')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clip_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defind  the image model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_model = lambda images: np.asarray(clip_model.encode_image(images.to(device)).float().detach().to('cpu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the needed libraries in the code \n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import os \n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defind a dataset class for images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class CustomDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, main_dir, transform):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        self.total_imgs = image_name_list\n",
        "        print(self.total_imgs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.total_imgs)\n",
        "\n",
        "    def get_image_name(self, idx):\n",
        "\n",
        "        return self.total_imgs[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
        "        image = Image.open(img_loc)\n",
        "\n",
        "        return self.transform(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defind a dataset class for text dataset  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts):\n",
        "        \"\"\"Define  the class init\"\"\"\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the text dataset\"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get the item based on index\"\"\"\n",
        "        return self.texts[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_encoder(text):\n",
        "    \"\"\"Normalize the text embeddings\"\"\"\n",
        "    embedding = language_model(text)\n",
        "    embedding = embedding / np.linalg.norm(embedding)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "def precompute_text_features(loader):\n",
        "    \"\"\"Compute the text embeddings of the whole dataset based on the loader provided\"\"\"\n",
        "    text_features = []\n",
        "\n",
        "    for _, (texts) in enumerate(tqdm(loader)):\n",
        "\n",
        "        embedding = language_model(texts)\n",
        "        embedding = embedding / np.linalg.norm(embedding)\n",
        "\n",
        "        text_features.extend(embedding)\n",
        "\n",
        "    return np.array(text_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precompute_image_features(loader):\n",
        "    image_features = []\n",
        "    \n",
        "    for i, (images) in enumerate(tqdm(loader)):\n",
        "\n",
        "        features = image_model(images)\n",
        "\n",
        "        features = features / np.linalg.norm(features)\n",
        "        image_features.extend(features)\n",
        "\n",
        "    return np.array(image_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_images(image_list):\n",
        "    for im_path in image_list:\n",
        "        print(im_path)\n",
        "        display(Image.open(im_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = 'بجعة تطفو أسفل النهر بالقارب'\n",
        "\n",
        "# image_paths = find_image(text, dataset, image_features, n=3)\n",
        "# show_images(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the image dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = CustomDataSet(\"photos/XTD10_dataset\", transform=compose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if the image_paths sorted_data in the same order of the image dataset:\n",
        "\n",
        "\n",
        "for i, item in enumerate(sorted_data):\n",
        "\n",
        "    if item['image_name'] != dataset.get_image_name(i):\n",
        "        print(\"stop\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the image_loder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the text_loder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_dataset = SimpleTextDataset([elem[\"caption_ar\"] for elem in sorted_data])\n",
        "\n",
        "text_loader = torch.utils.data.DataLoader(\n",
        "    text_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check this to utalize the GPU memory in the images \n",
        "# https://discuss.pytorch.org/t/not-using-multiprocessing-but-getting-cuda-error-re-forked-subprocess/54610/8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_features = precompute_image_features(image_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_emb_path = 'image_features.pickle'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_emb_path = 'text_features.pickle'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "with open(image_emb_path, 'wb') as handle:\n",
        "    pickle.dump(image_features, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(image_emb_path, 'rb') as handle:\n",
        "    image_features_new = pickle.load(handle)\n",
        "\n",
        "image_features_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_features = precompute_text_features(text_loader)\n",
        "\n",
        "text_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "with open(text_emb_path, 'wb') as handle:\n",
        "    pickle.dump(text_features, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open(text_emb_path, 'rb') as handle:\n",
        "    text_features_new = pickle.load(handle)\n",
        "\n",
        "text_features_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_features_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_features_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_features_new[0][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_features_new[0][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Take a look later over this\n",
        "\n",
        "# logit_scale = clip_model.logit_scale.exp().float().detach().to('cpu')\n",
        "# print(logit_scale)\n",
        "# logit_scale * text_features_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# logit_scale_val = logit_scale.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_path_coco(image_id):\n",
        "    # image_id = int(image_id)\n",
        "    # print(type(image_id))\n",
        "\n",
        "    im_path = id2path[image_id]\n",
        "    \n",
        "    return f\"photos/XTD10_dataset/{im_path}\" # f\"photos/val2014/COCO_val2014_{image_id:012d}.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat_indx_mrr = np.zeros((1000,1000),dtype=np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat_indx_mrr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mat_indx_mrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "collect_rr_testing = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check which axis the for loop get back\n",
        "# So, it loop over the raws\n",
        "\n",
        "chck_found = np.random.randint(10, size=(2, 4))\n",
        "for index, distances in enumerate(chck_found):\n",
        "    print(index)\n",
        "    print(distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the scores  \n",
        "\n",
        "text_features_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_features_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_embeddings(logit_scale, img_embs, txt_embs):\n",
        "  # normalized features\n",
        "  image_features = img_embs / img_embs.norm(dim=-1, keepdim=True)\n",
        "  text_features = txt_embs / txt_embs.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "  # logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "\n",
        "  logits_per_text = logit_scale * text_features @ image_features.t()\n",
        "\n",
        "  # print(\"type: \", type(logits_per_text))\n",
        "  \n",
        "  return logits_per_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # https://github.com/gpleiss/temperature_scaling\n",
        "# # CLIP Temperature scaler\n",
        "# logit_scale = clip_model.logit_scale.exp().float().to('cpu')\n",
        "\n",
        "# print(logit_scale)\n",
        "\n",
        "# language_logits = {}\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "# language_logits[\"Arabic\"] = compare_embeddings(logit_scale, torch.from_numpy(image_features_new), torch.from_numpy(text_features_new))\n",
        "# language_logits[\"Arabic\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_data[400+25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_data[400+86]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trial_1 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mrr(data, dataset, n):\n",
        "    \"\"\"Compute the MRR for the data based on n\"\"\"\n",
        "    collect_rr = []\n",
        "    pbar = tqdm(total=len(data), position=0, leave=True)\n",
        "\n",
        "    # print(\"text_features\")\n",
        "    # print(text_features)\n",
        "    # print(\"image_features\")\n",
        "    # print(image_features)\n",
        "\n",
        "    # print(\"image_features shape: \")\n",
        "    # print(image_features.shape)\n",
        "    # print()\n",
        "    # print(\"text_features shape: \")\n",
        "    # print(text_features.shape)\n",
        "    # found = np.matmul(text_features, image_features.T)\n",
        "    found = np.matmul(text_features_new, image_features_new.T)\n",
        "\n",
        "    # # instead: first shift the values of f so that the highest number is 0:\n",
        "    # found -= np.max(found)\n",
        "    # found_scalled = np.exp(found) / np.sum(np.exp(found)) # safe to do, gives the correct answer\n",
        "\n",
        "\n",
        "    # found_scalled = softmax(found) # .softmax(dim=-1).cpu().detach().numpy()\n",
        "    # print(\"print the matrix for the text features and the images featutes maltiplication found\")\n",
        "\n",
        "    # print(found)\n",
        "\n",
        "    for index, distances in enumerate(found): # It return the rows, one by one\n",
        "\n",
        "        pbar.update(1)\n",
        "        # print()\n",
        "        # print(\"index: \", index)\n",
        "        # print(\"data[index]['id']: inside the loop\", data[index][\"id\"])\n",
        "        image_path = get_path_coco(data[index][\"id\"])\n",
        "        # print(data[index][\"id\"])\n",
        "        # print(\"New link\")\n",
        "        # print(\"image_path in compute_mrr \", image_path)\n",
        "        # print(\"caption: \", data[index][\"caption\"])\n",
        "        # print(\"distances\")\n",
        "        # print(distances)\n",
        "        # print(\"n: \", n)\n",
        "\n",
        "        \n",
        "        collect_rr.append(new_rr(distances, image_path, dataset, n,index))\n",
        "\n",
        "\n",
        "    pbar.close()\n",
        "    print(100*\"=\")\n",
        "    # trial_1 = collect_rr.copy()\n",
        "    # print(collect_rr)\n",
        "    \n",
        "    return np.average(collect_rr)\n",
        "\n",
        "\n",
        "def new_rr(distances, target_image, dataset, n):\n",
        "    \"\"\"Calculate the RR for the given target image\"\"\"\n",
        "    image_paths = []\n",
        "\n",
        "    # print(\"distances: \", distances)\n",
        "    # print(\"type(distances): \", type(distances))\n",
        "    idxs = distances.argsort()[-n:][::-1] # Get the indcies for the images distances based on n\n",
        "\n",
        "    # print(idxs)\n",
        "        \n",
        "    # print(type(idxs))\n",
        "\n",
        "    # idxs = distances.argsort()[-n:][::-1] # Get the indcies for the images distances based on n\n",
        "    \n",
        "    # print(\"distances.argsort(): \", distances.argsort())\n",
        "    # print(\"distances.argsort()[-n:]: \", distances.argsort()[-n:])\n",
        "    # print(\"distances.argsort()[-n:][::-1]: \", distances.argsort()[-n:][::-1])\n",
        "\n",
        "    # print(\"idxs of the images from the top to the lower: \", idxs)\n",
        "    # print(\"target_image: \", target_image)\n",
        "    for idx in idxs:\n",
        "        # print(\"'photos/val2014/' + dataset.get_image_name(idx): \", 'photos/val2014/' + dataset.get_image_name(idx))\n",
        "        image_paths.append('photos/XTD10_dataset/' + dataset.get_image_name(idx))\n",
        "        # image_paths.append(get_path_coco(data[idx][\"id\"]))\n",
        "\n",
        "    # print(\"target_image: \", target_image)\n",
        "    # print(\"image_paths: \", image_paths)\n",
        "\n",
        "    if target_image in image_paths:\n",
        "\n",
        "        return 1/(image_paths.index(target_image) + 1)\n",
        "    else:\n",
        "        # print(\"new_rr: \", 0)\n",
        "        return 0\n",
        "\n",
        "\n",
        "def internal_hits(distances, target_image, dataset, n):\n",
        "    \"\"\"Calculate the hits of the target images based on the existance of it or not\"\"\"\n",
        "    image_paths = []\n",
        "    idxs = distances.argsort()[-n:][::-1]\n",
        "\n",
        "    if target_image in idxs:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def compute_hits(data, dataset, n):\n",
        "\n",
        "    index_cnt = 0\n",
        "\n",
        "    collect_rr = []\n",
        "\n",
        "    pbar = tqdm(total=len(data), position=0, leave=True)\n",
        "\n",
        "    found = np.matmul(text_features_new, image_features_new.T)\n",
        "\n",
        "    for index, distances in enumerate(found):\n",
        "        pbar.update(1)\n",
        "        # image_path = get_path_coco(data[index][\"id\"])\n",
        "        image_path = index # get_path_coco(data[index][\"id\"])\n",
        "        \n",
        "        collect_rr.append(internal_hits(distances, image_path, dataset, n))\n",
        "        # collect_rr_testing.append(internal_hits(distances, image_path, dataset, n))\n",
        "        break\n",
        "\n",
        "    \n",
        "    pbar.close()\n",
        "    # print(len(collect_rr_testing))\n",
        "    return np.average(collect_rr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def compute_mrr(data, dataset, n):\n",
        "#     \"\"\"Compute the MRR for the data based on n\"\"\"\n",
        "#     collect_rr = []\n",
        "\n",
        "#     found = np.matmul(text_features, image_features.T)\n",
        "\n",
        "\n",
        "#     for index, cos_vlaues in enumerate(found):\n",
        "\n",
        "#         image_path = get_image_path(data[index][\"id\"])\n",
        "\n",
        "#         result = 0\n",
        "\n",
        "#         image_paths = []\n",
        "\n",
        "#         idxs = cos_vlaues.argsort()[-n:][::-1] \n",
        "        \n",
        "#         for idx in idxs:\n",
        "#             image_paths.append(get_image_path(idx))\n",
        "\n",
        "#         if target_image in image_paths:\n",
        "\n",
        "#             result = 1/(image_paths.index(target_image) + 1)\n",
        "\n",
        "#         collect_rr.append(result)\n",
        "\n",
        "#     return np.average(collect_rr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mrr(data, dataset, n):\n",
        "    \"\"\"Compute the MRR for the data based on n\"\"\"\n",
        "    collect_rr = []\n",
        "\n",
        "    found = np.matmul(text_features_new, image_features_new.T)\n",
        "    for index, distances in enumerate(found): # It return the rows, one by one\n",
        "\n",
        "        image_path = get_path_coco(data[index][\"id\"])\n",
        "        collect_rr.append(new_rr(distances, image_path, dataset, n,index))\n",
        "\n",
        "        \n",
        "\n",
        "    return np.average(collect_rr)\n",
        "\n",
        "def new_rr(distances, target_image, dataset, n,index):\n",
        "    \"\"\"Calculate the RR for the given target image\"\"\"\n",
        "    image_paths = []\n",
        "\n",
        "    idxs = distances.argsort()[-n:][::-1] \n",
        "    \n",
        "    # print(\"target_image: \", target_image)\n",
        "    \n",
        "\n",
        "    for idx in idxs:\n",
        "        image_paths.append('photos/XTD10_dataset/' + dataset.get_image_name(idx))\n",
        "    \n",
        "\n",
        "    # print(\"image_paths: \", image_paths)\n",
        "    \n",
        "    if target_image in image_paths:\n",
        "\n",
        "        return 1/(image_paths.index(target_image) + 1)\n",
        "    else:\n",
        "        return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # image_encoder - ResNet or Vision Transformer\n",
        "# # text_encoder - CBOW or Text Transformer\n",
        "# # I[n, h, w, c] - minibatch of aligned images\n",
        "# # T[n, l] - minibatch of aligned texts\n",
        "# # W_i[d_i, d_e] - learned proj of image to embed\n",
        "# # W_t[d_t, d_e] - learned proj of text to embed\n",
        "# # t - learned temperature parameter\n",
        "# # extract feature representations of each modality\n",
        "# I_f = image_encoder(I) #[n, d_i]\n",
        "# T_f = text_encoder(T) #[n, d_t]\n",
        "# # joint multimodal embedding [n, d_e]\n",
        "# I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
        "# T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
        "# # scaled pairwise cosine similarities [n, n]\n",
        "# logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
        "# # symmetric loss function\n",
        "# labels = np.arange(n)\n",
        "# loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
        "# loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
        "# loss = (loss_i + loss_t)/2\n",
        "\n",
        "\n",
        "# Figure 3. Numpy-like pseudocode for the core of an implementa-\n",
        "# tion of CLIP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('MRR@1:', compute_mrr(sorted_data, dataset, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('MRR@5:', compute_mrr(sorted_data, dataset, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('MRR@10:', compute_mrr(sorted_data, dataset,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(compute_hits(sorted_data, dataset, 1)* 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(compute_hits(sorted_data, dataset, 5)* 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(compute_hits(sorted_data, dataset, 10)* 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation based on Recall metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_features_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_features_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_features_new_pt = torch.from_numpy(image_features_new)\n",
        "\n",
        "text_features_new_pt = torch.from_numpy(text_features_new)\n",
        "\n",
        "text_to_image_map = torch.LongTensor(list(range(text_features_new.shape[0])))\n",
        "print(text_to_image_map.shape) # .type(torch.int64)\n",
        "\n",
        "print(text_to_image_map.unsqueeze(1).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.set_printoptions(precision=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://github.com/openai/CLIP/issues/115\n",
        "import torch\n",
        "from torchvision.datasets import CocoCaptions\n",
        "import torch.utils.data as dutils\n",
        "from typing import List\n",
        "import clip\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def recall_at_k(k_vals, image_encodings,text_encodings,text_to_image_map):\n",
        "    print(\"Encoding all data...\")\n",
        " \n",
        "    num_text = text_encodings.shape[0]\n",
        "    \n",
        "    # text-to-image recall\n",
        "    print(\"Text-to-image recall...\")\n",
        "\n",
        "\n",
        "    dist_matrix = text_encodings @ image_encodings.T  # dist_matrix[i] gives logits for ith text\n",
        "\n",
        "    inds = torch.argsort(dist_matrix, dim=1, descending=True)\n",
        "    inds = inds.to(device)\n",
        "    text_to_image_recall = []\n",
        "\n",
        "    \n",
        "\n",
        "    text_to_image_map = text_to_image_map.to(device)\n",
        "    \n",
        "    for k in k_vals:\n",
        "        # Extract top k indices only\n",
        "        topk = inds[:, :k]\n",
        "\n",
        "        text_to_image_map_new = text_to_image_map.repeat(k, 1).t()\n",
        "\n",
        "        correct = torch.eq(topk, text_to_image_map_new).any(dim=1)  #  value along dimension 1 (which typically corresponds to rows in a 2D tensor) ###### any(dim=1) >> check if True over the row \n",
        "        \n",
        "        num_correct = correct.sum().item()\n",
        "\n",
        "        text_to_image_recall.append(num_correct / num_text)\n",
        "\n",
        "    print(text_to_image_recall)\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return text_to_image_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_vals = [1,5,10]\n",
        "t2i= recall_at_k(k_vals=k_vals, image_encodings=image_features_new_pt,text_encodings=text_features_new_pt,text_to_image_map=text_to_image_map)\n",
        "\n",
        "print(\"Text-to-image Recall@K\")\n",
        "\n",
        "print(\"Returned value: \", t2i)\n",
        "for k, x in zip(k_vals, t2i):\n",
        "    print(k, \" \", (x/100) * 100)\n",
        "    # print(f\" R@{k}: {100*x:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Multilingual_CLIP.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
